\documentclass[12pt, a4paper]{article}
\usepackage{blindtext, titlesec, amsthm, thmtools, amsmath, amsfonts, scalerel, amssymb, graphicx, titlesec, xcolor, multicol, mathtools}
%\usepackage{hyperref}
\usepackage[utf8]{inputenc}
%\hypersetup{colorlinks,linkcolor={red!40!black},citecolor={blue!50!black},urlcolor={blue!80!black}}
\newtheorem{theorem}{Theorema}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollarium}
\newtheorem{hypothesis}{Coniectura}
\theoremstyle{definition}
\newtheorem{definition}{Definitio}[section]
\theoremstyle{remark}
\newtheorem{remark}{Observatio}[section]
\newtheorem{example}{Exampli Gratia}[section]
\renewcommand\qedsymbol{Q.E.D.}
\title{On Linear Regression}
\author{Harry Han}
\date{\today}
\begin{document}
\maketitle

\section{The Simplest: $y=ax+b$}
\begin{definition}[$R^2$]
	
\end{definition}

For a data set $\vec{x}=[x_0, x_1, x_2, \cdots]^T$ and $\vec{y}=[y_0, y_1, y_2, \cdots]^T$ with the same dimension, 
there exist a linear regression $y=ax+b$ such that the $R^2$ value is minimized. 

The promised regression is:
\begin{equation}
\begin{multlined}
	a = \frac{m_2n_1+n_2}{1-m_2m_1}; 
	b = \frac{m_2n_1+n_2}{1/m_1-m_2}+n_1
	\\
	m_1 = -\frac{\sum \vec{x}}{n}; n_1 = \frac{\sum \vec{y}}{n}; m_2=-\frac{\sum \vec{x}}{|\vec{x}|^2}; n_2 = \frac{\vec{x}\cdot \vec{y}}{|\vec{x}|^2}
\end{multlined}
\end{equation}

Where $\sum \vec{x}$ denote the sum of all entries in the vector $\vec{x}$.

\end{document}

